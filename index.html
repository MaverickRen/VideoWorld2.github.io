<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VideoWorld 2: Learning Transferable Knowledge from Real-world Videos">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VideoWorld 2: Learning Transferable Knowledge from Real-world Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Exo+2:wght@300;400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/videoworld_logo.svg">
  <link rel="stylesheet" href="./static/css/float.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "HTML-CSS": { styles: { '.MathJax_Display': { color: "#c5c6c7" }, '.MathJax': { color: "#c5c6c7" } } },
      tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } 
    });
  </script>

  <style>
    /* --- åŸºç¡€æ¸…ç† --- */
    .results-carousel video {
      border: none !important;
      outline: none !important;
      box-shadow: none !important;
    }
    .results-carousel .item {
      border: none !important;
      outline: none !important;
    }

    /* --- å¯¹è¯æ¡†å¸ƒå±€æ ·å¼ --- */
    .dialogue-container {
      display: flex;
      flex-direction: column;
      width: 90%;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      min-height: 550px; 
      background-color: #151922;
      border-radius: 15px;
      position: relative;
    }

    /* å³ä¾§ï¼ˆç”¨æˆ·ï¼‰ */
    .dialogue-right {
      display: flex;
      flex-direction: column;
      align-items: flex-end;
      margin-bottom: 20px;
      width: 100%;
    }

    /* å·¦ä¾§ï¼ˆAIï¼‰ */
    .dialogue-left {
      display: flex;
      flex-direction: column;
      align-items: flex-start;
      width: 100%;
    }

    /* --- æš´åŠ›ä¿®å¤ï¼šå¼ºåˆ¶æ˜¾ç¤ºæ‰€æœ‰å†…å®¹ --- */
    .dialogue-box {
      opacity: 1 !important; /* å¼ºåˆ¶ä¸é€æ˜ */
      visibility: visible !important; /* å¼ºåˆ¶å¯è§ */
      display: block !important; /* å¼ºåˆ¶å—çº§æ˜¾ç¤º */
      margin-bottom: 15px;
    }

    /* å…·ä½“ç»„ä»¶æ ·å¼ */
    .user-box img {
      width: 180px;
      border-radius: 12px;
      border: 1px solid rgba(255, 255, 255, 0.2);
      box-shadow: 0 5px 15px rgba(0,0,0,0.3);
    }

    .user-text p {
      background-color: #007aff;
      color: white;
      padding: 12px 20px;
      border-radius: 18px 18px 0 18px;
      font-family: 'Google Sans', sans-serif;
      font-size: 18px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
      margin: 0;
    }

    .ai-text p {
      color: #b0b0b0;
      font-style: italic;
      font-family: 'Google Sans', sans-serif;
      font-size: 16px;
      margin-left: 5px;
      margin-bottom: 5px;
    }

    .ai-video video {
      width: 100%;
      max-width: 500px;
      border-radius: 12px;
      box-shadow: 0 10px 30px rgba(0,0,0,0.5); /* åŠ æ·±é˜´å½± */
      border: 1px solid rgba(255, 255, 255, 0.1);
      display: block;
    }

    /* --- é«˜çº§è§†è§‰ä¼˜åŒ– --- */
    
    /* å…¨å±€å­—ä½“ä¼˜åŒ– */
    body {
      font-family: 'Google Sans', 'Noto Sans', sans-serif;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
      background-color: transparent; /* é€æ˜ï¼Œè®©ä½äºä¼ªå…ƒç´ èƒŒæ™¯ */
      color: #f0f0f0;
      position: relative;
    }
    
    /* æ•´ä¸ªé¡µé¢çš„èƒŒæ™¯åº•è‰²ï¼Œé˜²æ­¢ä¼ªå…ƒç´ åŠ è½½å‰ç™½å± */
    html {
      background-color: #020205;
      height: 100%;
    }

    /* 1. æ·±ç©ºæ¸å˜å±‚ (å›ºå®šèƒŒæ™¯ï¼Œå¤åˆ» TechBackground) */
    body::before {
      content: "";
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      /* å‡çº§ç‰ˆï¼šä½¿ç”¨ ellipse (æ¤­åœ†) é€‚é…å®½å±æ˜¾ç¤ºå™¨ï¼Œè§£å†³â€œä¸¤ä¾§è¿‡é»‘â€çš„é—®é¢˜ */
      /* ç¨å¾®æäº®äº†ä¸­å¿ƒç´«è‰² (#262645)ï¼Œå¹¶è®©æ¸å˜èŒƒå›´å»¶ä¼¸è‡³ 120% ç”šè‡³æ›´è¿œï¼Œç¡®ä¿å…¨å±è¦†ç›– */
      background: radial-gradient(ellipse 100% 90% at 50% 15%, #262645 0%, #080810 50%, #000000 100%);
      z-index: -2;
    }

    /* 2. åŠ¨æ€ç½‘æ ¼å±‚ (å¤åˆ» TechBackground Grid) */
    body::after {
      content: "";
      position: fixed;
      top: 0;
      left: 0;
      width: 200%; /* è¶³å¤Ÿå¤§ä»¥ä¾¿ç§»åŠ¨ */
      height: 200%;
      /* æµ…è“è‰²æç»†ç½‘æ ¼ rgba(77, 171, 247, 0.05) */
      background: 
        linear-gradient(90deg, rgba(77, 171, 247, 0.05) 1px, transparent 1px),
        linear-gradient(rgba(77, 171, 247, 0.05) 1px, transparent 1px);
      background-size: 80px 80px;
      animation: grid-move 60s linear infinite;
      z-index: -1;
      pointer-events: none; /* ç¡®ä¿ä¸æŒ¡ä½ç‚¹å‡» */
    }

    @keyframes grid-move {
      0% { transform: translate(0, 0); }
      100% { transform: translate(-40px, -40px); }
    }

    ::-webkit-scrollbar {
      width: 10px;
    }
    ::-webkit-scrollbar-track {
      background: #0b0c10;
    }
    ::-webkit-scrollbar-thumb {
      background: #333;
      border-radius: 5px;
    }
    ::-webkit-scrollbar-thumb:hover {
      background: #00f2fe;
    }

    a {
      color: #4facfe;
      transition: all 0.3s ease;
    }
    a:hover {
      color: #ffffff;
      text-shadow: 0 0 8px rgba(255, 255, 255, 0.4);
    }

    /* æ ‡é¢˜å­—ä½“å¢å¼ºä¸é«˜çº§æ„Ÿè®¾è®¡ */
    h1.title, h2.title, .title {
      font-family: 'Exo 2', sans-serif !important; /* å‚è€ƒ TSX: ä½¿ç”¨å…·æœ‰ç§‘æŠ€æ„Ÿçš„ Exo 2 å­—ä½“ */
      letter-spacing: 0.02em;
      color: #ffffff !important;
      text-shadow: none; /* ä¿æŒç®€æ´ï¼Œå»é™¤è¿‡å¤šå…‰æ•ˆ */
      position: relative;
    }

    /* ä¸»æ ‡é¢˜ï¼šå¾®ç«‹ä½“æ„Ÿ */
    .publication-title {
      text-shadow: none;
    }

    /* ç« èŠ‚æ ‡é¢˜è£…é¥°ï¼šåŠ¨æ€å…‰çº¿ */
    h2.title {
      display: inline-block;
      padding-bottom: 12px; /* ä¸ºæ¨ªçº¿ç•™å‡ºç©ºé—´ */
    }
    
    h2.title::after {
      content: '';
      position: absolute;
      bottom: 0;
      left: 50%;
      transform: translateX(-50%);
      width: 40px; /* åˆå§‹çŸ­æ¨ªçº¿ï¼Œæç®€é£æ ¼ */
      height: 2px;
      background: #ffffff; /* çº¯ç™½çº¿æ¡ï¼Œç®€æ´ */
      border-radius: 4px;
      transition: width 0.4s cubic-bezier(0.25, 0.8, 0.25, 1);
      box-shadow: none;
    }

    /* --- å¼ºåˆ¶è¦†ç›–æ—§å­—ä½“ --- */
    font[face="Georgia"] {
      font-family: 'Google Sans', sans-serif !important;
    }
    
    /* å¯¹æ‰€æœ‰æ˜¾å¼è®¾ç½®äº† Georgia çš„å…ƒç´ è¿›è¡Œè¦†ç›–ï¼Œæ”¹ä¸ºæ— è¡¬çº¿ä½“ */
    [style*="font-family: 'Georgia'"] {
      font-family: 'Exo 2', sans-serif !important;
    }
    
    /* ç‰¹åˆ«ä¿®æ­£æ­£æ–‡åŒºåŸŸçš„å­—ä½“ï¼Œä¸ä½¿ç”¨ Exo 2 è€Œæ˜¯ç”¨æ›´å¥½è¯»çš„ Google Sans */
    .content [style*="font-family: 'Georgia'"],
    .publication-authors [style*="font-family: 'Georgia'"] {
      font-family: 'Google Sans', sans-serif !important;
    }

    /* é¼ æ ‡æ‚¬åœæ—¶ä¼˜é›…å±•å¼€ */
    h2.title:hover::after {
      width: 60%; 
    }

    /* æŒ‰é’®é«˜çº§åŠ¨æ•ˆ - ç»ç’ƒæ‹Ÿæ€ç§‘æŠ€æ„Ÿ */
    .button.is-dark {
      background-color: rgba(255, 255, 255, 0.05); /* åŠé€æ˜ */
      backdrop-filter: blur(5px);
      color: #f0f0f0;
      transition: all 0.3s ease;
      border: 1px solid rgba(255, 255, 255, 0.1);
      box-shadow: 0 4px 6px rgba(0,0,0,0.2);
    }
    .button.is-dark:hover {
      background-color: rgba(255, 255, 255, 0.15) !important;
      transform: translateY(-2px);
      box-shadow: 0 0 20px rgba(79, 172, 254, 0.3); /* é’è‰²å‘¼å¸å…‰æ™• */
      border-color: #4facfe;
      color: #ffffff;
    }

    /* æ»šåŠ¨å‡ºç°åŠ¨ç”» */
    .scroll-reveal {
      opacity: 0;
      transform: translateY(30px);
      transition: all 0.8s cubic-bezier(0.5, 0, 0, 1);
    }
    .scroll-reveal.visible {
      opacity: 1;
      transform: translateY(0);
    }

    /* Highlights å¡ç‰‡åŒ–ä¼˜åŒ– - ç»ç’ƒæ‹Ÿæ€ */
    .highlight-box {
      background: rgba(30, 30, 45, 0.4); /* æ·±è‰²åŠé€æ˜ */
      backdrop-filter: blur(10px);
      -webkit-backdrop-filter: blur(10px);
      padding: 25px;
      border-radius: 12px;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
      margin-bottom: 20px;
      border: 1px solid rgba(255, 255, 255, 0.08); 
      border-left: 5px solid #4facfe; /* é’è‰²å¼ºè°ƒï¼Œå‘¼åº”é“¾æ¥è‰² */
      transition: transform 0.3s ease, box-shadow 0.3s ease, background-color 0.3s;
    }
    .highlight-box:hover {
      background: rgba(40, 40, 60, 0.6);
      transform: translateY(-3px);
      box-shadow: 0 15px 40px rgba(0,0,0,0.5);
      border-color: rgba(255, 255, 255, 0.2);
    }

    /* å¯¹è¯æ¡†é«˜çº§è´¨æ„Ÿ - æ·±åº¦ç»ç’ƒæ‹Ÿæ€ */
    .dialogue-container {
      display: flex;
      flex-direction: column;
      width: 90%;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      min-height: 550px; 
      background: rgba(20, 20, 30, 0.6);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      box-shadow: 0 20px 50px rgba(0,0,0,0.6);
      border: 1px solid rgba(255, 255, 255, 0.08); /* ç»Ÿä¸€è¾¹æ¡†é£æ ¼ */
      border-radius: 15px;
      position: relative;
    }
    
    .footer {
      background-color: transparent;
      color: #8b949e;
      padding-bottom: 3rem;
    }
    
    .user-text p {
      background: linear-gradient(135deg, #2b32b2 10%, #1488cc 100%); /* æ·±è“ç´«åˆ°é’è‰²æ¸å˜ */
      border: 1px solid rgba(255, 255, 255, 0.1);
      /* ...existing code... */
    }

    /* ç«ç„°è·³åŠ¨åŠ¨ç”» */
    @keyframes flame-pulse {
      0% { transform: scale(1); }
      50% { transform: scale(1.2) rotate(5deg); }
      100% { transform: scale(1); }
    }
    .fire-icon {
      display: inline-block;
      animation: flame-pulse 2s infinite ease-in-out;
    }

    /* Logo æ ·å¼ */
    .project-logo {
      transition: transform 0.4s cubic-bezier(0.34, 1.56, 0.64, 1), filter 0.4s ease;
      filter: drop-shadow(0 0 10px rgba(0, 242, 254, 0.4)); /* ç§‘æŠ€å…‰ç›¾ */
      cursor: pointer;
      animation: logo-intro 1.2s cubic-bezier(0.34, 1.56, 0.64, 1) 0.5s;
    }
    
    /* ä¸»æ ‡é¢˜æ–‡å­—ç‰¹æ•ˆ */
    .main-title-text {
      display: inline-block;
      transition: transform 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275), filter 0.3s ease;
      cursor: default;
      animation: title-intro 1.2s cubic-bezier(0.175, 0.885, 0.32, 1.275) 0.5s;
    }

    /* é¼ æ ‡æ‚¬åœåœ¨ä¸»æ ‡é¢˜æ•´ä½“æ—¶è§¦å‘ç‰¹æ•ˆ */
    .publication-title:hover .project-logo {
      transform: scale(1.15) rotate(-3deg);
      filter: drop-shadow(0 10px 20px rgba(255, 255, 255, 0.3));
    }

    .publication-title:hover .main-title-text {
      transform: scale(1.05) translateY(-2px);
      filter: brightness(1.2);
    }

    /* --- å¼€åœºåŠ¨ç”» Keyframes --- */
    @keyframes logo-intro {
      0% { transform: scale(1) rotate(0deg); filter: drop-shadow(0 2px 4px rgba(0,0,0,0.1)); }
      50% { transform: scale(1.15) rotate(-3deg); filter: drop-shadow(0 10px 20px rgba(255, 255, 255, 0.3)); }
      100% { transform: scale(1) rotate(0deg); filter: drop-shadow(0 2px 4px rgba(0,0,0,0.1)); }
    }

    @keyframes title-intro {
      0% { transform: scale(1) translateY(0); filter: brightness(1) hue-rotate(0deg); }
      50% { transform: scale(1.05) translateY(-2px); filter: brightness(1.2) hue-rotate(-10deg); }
      100% { transform: scale(1) translateY(0); filter: brightness(1) hue-rotate(0deg); }
    }

    /* --- å°æ ‡é¢˜ç‰¹æ•ˆ (å¤åˆ»å¤§æ ‡é¢˜) --- */
    h2.title .section-logo {
      transition: transform 0.4s cubic-bezier(0.34, 1.56, 0.64, 1), filter 0.4s ease;
      filter: drop-shadow(0 0 8px rgba(0, 242, 254, 0.3));
      display: inline-block;
      vertical-align: middle;
      margin-right: 10px; /* è°ƒæ•´å›¾æ ‡å’Œæ–‡å­—é—´è· */
    }
    
    h2.title .section-text {
      display: inline-block;
      transition: transform 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275), filter 0.3s ease;
      vertical-align: middle;
    }

    /* é¼ æ ‡æ‚¬åœåœ¨æ ‡é¢˜æ•´ä½“ï¼ˆåŒ…æ‹¬æ–‡å­—ï¼‰æ—¶è§¦å‘ç‰¹æ•ˆ */
    h2.title:hover .section-logo {
      transform: scale(1.15) rotate(-3deg);
      filter: drop-shadow(0 10px 20px rgba(255, 255, 255, 0.3));
    }

    h2.title:hover .section-text {
      transform: scale(1.05) translateY(-2px);
      filter: brightness(1.2);
    }

    /* --- å‰¯æ ‡é¢˜ç‰¹æ•ˆ --- */
    .subtitle-text {
      opacity: 0;
      animation: fadeInUp 1s ease-out 0.5s forwards; /* å»¶è¿Ÿå‡ºç° */
      transition: color 0.3s ease, text-shadow 0.3s ease, transform 0.3s ease;
      display: inline-block;
    }

    .subtitle-text:hover {
      color: #ffffff !important; /* æ‚¬åœå˜ç™½ */
      text-shadow: 0 0 10px rgba(255, 255, 255, 0.4);
      transform: scale(1.02);
    }

    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(10px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    /* BibTeX ä»£ç å—æ ·å¼ä¼˜åŒ– - ç»ç’ƒæ‹Ÿæ€ */
    #BibTeX pre {
      background-color: rgba(20, 20, 30, 0.6);
      backdrop-filter: blur(5px);
      -webkit-backdrop-filter: blur(5px);
      border: 1px solid rgba(255, 255, 255, 0.1);
      border-radius: 8px;
      padding: 1.5rem;
    }
    
    #BibTeX code {
      background-color: transparent;
      color: #b0b0b0;
      font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
      font-size: 14px;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="display: flex; flex-direction: column; align-items: center;">
            <div style="display: flex; align-items: center; gap: 15px; margin-bottom: 10px;">
              <img src="./static/images/videoworld_logo.svg" style="width: 60px; height: auto;" alt="Logo" class="project-logo"/>
              <span class="main-title-text" style="font-family: 'Exo 2', sans-serif; font-weight: 700; color: #f0f0f0;">VideoWorld 2</span>
            </div>
            <span class="subtitle-text" style="font-family: 'Exo 2', sans-serif; font-size: 0.6em; color: #ffffff; font-weight: 400; letter-spacing: 0.5px;">Learning Transferable Knowledge from Real-world Videos</span>
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=e5TJm-0AAAAJ&hl=zh-CN"><font face="Georgia">Zhongwei Ren</font></a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://weiyc.github.io/"><font face="Georgia">Yunchao Wei</font></a><sup>1<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup>,
              <span class="author-block">
                <a href="https://weiyc.github.io/"><font face="Georgia">Xiao Yu</font></a><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://weiyc.github.io/"><font face="Georgia">Guixun Luo</font></a><sup>1<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup>,

            </span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=474TbQYAAAAJ&hl=zh-CN"><font face="Georgia">Yao Zhao</font></a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=NmHgX-wAAAAJ&hl=en"><font face="Georgia">Bingyi Kang</font></a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/jshfeng/home"><font face="Georgia">Jiashi Feng</font></a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en"><font face="Georgia">Xiaojie Jin</font></a><sup>2<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€¡</mo></math></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup><font face="Georgia">Beijing Jiaotong University,</font></span>
            <span class="author-block"><sup>2</sup><font face="Georgia">ByteDance Seed</font></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(<sup><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup><font face="Georgia">Correspondence,</font><sup><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€¡</mo></math></sup><font face="Georgia">Project Lead)</font></span>
          </div>

         
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2501.09781"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2501.09781"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/Cdi-qikbEzk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/bytedance/VideoWorld"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-full">
    <div class="hero-body">
      <img src="./static/images/Fig1_final.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>

                <div class="columns is-centered has-text-centered">
                  <div class="column is-full">
                    <div class="content has-text-justified">
                     
                      <p style="font-size: 16px;">
                        <font face="Georgia">Figure 1: (left) VideoWorld 2 explores how to learn transferable knowledge from unlabeled real-world videos. (right) Comparison of different frameworks. VDM (e.g., Wan2.2 14B) produces high visual fidelity but fails to learn task-relevant dynamics or long-horizon policies. VideoWorld 1 improves policy learning but suffers from poor visual quality in real-world scenarios. VideoWorld 2 learns more robust latent dynamics while also achieving significantly better visual quality, enabling generalizable long-horizon knowledge learning from videos.</font>
                      </p>
                    </div>
                  </div>
                </div>
      </div>
    </div>
  </div>
</section>

<!-- Video Carousel Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">
        <img src="./static/images/videoworld_logo.svg" class="section-logo" alt="Logo" width="5%"/>
        <span class="section-text" style="font-family: 'Georgia', serif;">Video Demos</span>
      </h2>
      <div id="results-carousel" class="carousel results-carousel">
        
        <!-- Item 1: Paper Airplane -->
        <div class="item item-video1">
          <div class="dialogue-container">
            <!-- Right Side: User Instruction -->
            <div class="dialogue-right">
              <div class="dialogue-box user-box">
                <img src="./static/images/Paper_Airplane_Example_1.png" alt="Instruction Image">
              </div>
              <div class="dialogue-box user-text">
                <p>Please fold a paper airplane</p>
              </div>
            </div>

            <!-- Left Side: AI Response -->
            <div class="dialogue-left">
              <div class="dialogue-box ai-text">
                <p>Imagining...</p>
              </div>
              <div class="dialogue-box ai-video">
                <video poster="" id="video1" controls muted loop playsinline preload="metadata">
                  <source src="./static/images/Paper_Airplane_Example_1.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>
        
        <div class="item item-video1">
          <div class="dialogue-container">
            <!-- Right Side: User Instruction -->
            <div class="dialogue-right">
              <div class="dialogue-box user-box">
                <img src="./static/images/Paper_Airplane_Example_2.png" alt="Instruction Image">
              </div>
              <div class="dialogue-box user-text">
                <p>Please fold a paper airplane</p>
              </div>
            </div>

            <!-- Left Side: AI Response -->
            <div class="dialogue-left">
              <div class="dialogue-box ai-text">
                <p>Imagining...</p>
              </div>
              <div class="dialogue-box ai-video">
                <video poster="" id="video1" controls muted loop playsinline preload="metadata">
                  <source src="./static/images/Paper_Airplane_Example_2.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <div class="item item-video1">
          <div class="dialogue-container">
            <!-- Right Side: User Instruction -->
            <div class="dialogue-right">
              <div class="dialogue-box user-box">
                <img src="./static/images/BuildingBlock_Person_Example_1.png" alt="Instruction Image">
              </div>
              <div class="dialogue-box user-text">
                <p>Please build a person using blocks</p>
              </div>
            </div>

            <!-- Left Side: AI Response -->
            <div class="dialogue-left">
              <div class="dialogue-box ai-text">
                <p>Imagining...</p>
              </div>
              <div class="dialogue-box ai-video">
                <video poster="" id="video1" controls muted loop playsinline preload="metadata">
                  <source src="./static/images/BuildingBlock_Person_Example_1.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <div class="item item-video1">
          <div class="dialogue-container">
            <!-- Right Side: User Instruction -->
            <div class="dialogue-right">
              <div class="dialogue-box user-box">
                <img src="./static/images/BuildingBlock_Tower_Example_1.png" alt="Instruction Image">
              </div>
              <div class="dialogue-box user-text">
                <p>Please build a tower using blocks</p>
              </div>
            </div>

            <!-- Left Side: AI Response -->
            <div class="dialogue-left">
              <div class="dialogue-box ai-text">
                <p>Imagining...</p>
              </div>
              <div class="dialogue-box ai-video">
                <video poster="" id="video1" controls muted loop playsinline preload="metadata">
                  <source src="./static/images/BuildingBlock_Tower_Example_1.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <div class="item item-video1">
          <div class="dialogue-container">
            <!-- Right Side: User Instruction -->
            <div class="dialogue-right">
              <div class="dialogue-box user-box">
                <img src="./static/images/Paper_Boat_Example_2.png" alt="Instruction Image">
              </div>
              <div class="dialogue-box user-text">
                <p>Please fold a paper boat</p>
              </div>
            </div>

            <!-- Left Side: AI Response -->
            <div class="dialogue-left">
              <div class="dialogue-box ai-text">
                <p>Imagining...</p>
              </div>
              <div class="dialogue-box ai-video">
                <video poster="" id="video1" controls muted loop playsinline preload="metadata">
                  <source src="./static/images/Paper_Boat_Example_2.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <div class="item item-video1">
          <div class="dialogue-container">
            <!-- Right Side: User Instruction -->
            <div class="dialogue-right">
              <div class="dialogue-box user-box">
                <img src="./static/images/BuildingBlock_Tower_Example_2.png" alt="Instruction Image">
              </div>
              <div class="dialogue-box user-text">
                <p>Please fold a tower airplane</p>
              </div>
            </div>

            <!-- Left Side: AI Response -->
            <div class="dialogue-left">
              <div class="dialogue-box ai-text">
                <p>Imagining...</p>
              </div>
              <div class="dialogue-box ai-video">
                <video poster="" id="video1" controls muted loop playsinline preload="metadata">
                  <source src="./static/images/BuildingBlock_Tower_Example_2.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <!-- Item 2: Building Block Horse -->
        <div class="item item-video2">
           <div class="dialogue-container">
            <div class="dialogue-right">
              <div class="dialogue-box user-box">
                <img src="./static/images/BuildingBlock_Horse_Example_1.png" alt="Instruction Image"> 
              </div>
              <div class="dialogue-box user-text">
                <p>Please build a horse</p>
              </div>
            </div>
            <div class="dialogue-left">
              <div class="dialogue-box ai-text">
                <p>Imagining...</p>
              </div>
              <div class="dialogue-box ai-video">
                <video poster="" id="video2" controls muted loop playsinline preload="metadata">
                  <source src="./static/images/BuildingBlock_Horse_Example_1.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <!-- Item 3: Paper Boat -->
        <div class="item item-video3">
           <div class="dialogue-container">
            <div class="dialogue-right">
              <div class="dialogue-box user-box">
                <img src="./static/images/Paper_Boat_Example_1.png" alt="Instruction Image">
              </div>
              <div class="dialogue-box user-text">
                <p>Please fold a paper boat</p>
              </div>
            </div>
            <div class="dialogue-left">
              <div class="dialogue-box ai-text">
                <p>Imagining...</p>
              </div>
              <div class="dialogue-box ai-video">
                <video poster="" id="video3" controls muted loop playsinline preload="metadata">
                  <source src="./static/images/Paper_Boat_Example_1.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">
          <img src="./static/images/videoworld_logo.svg" class="section-logo" alt="Logo" width="6%"/>
          <span class="section-text" style="font-family: 'Georgia', serif;">Abstract</span>
        </h2>
        <div class="content has-text-justified">
          <p style="font-size: 20px;">
            <font face="Georgia">Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents <b>VideoWorld 2</b>, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a disentangled Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to <b>70% improvement in task success rate</b> and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.</font>
          </p>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <!-- <h2 class="title is-3"></h2> -->
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/y_TT4dtIPXA?si=jiaM-BjxEeT2WLL5"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="hero is-big" style="margin-top: 35px; padding-bottom: 35px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <br>
        <h2 class="title is-3"><span class="fire-icon">ğŸ”¥</span><span class="section-text" style="font-family: 'Georgia', serif;">Highlights</span> </h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full" style="font-family: 'Georgia', serif;">
          
          <div class="highlight-box">
            <p style="font-size: 20px; margin-bottom: 0;">
              1. <b>We are the first to explore</b> <span style="color: #00f2fe">how to learn transferable world knowledge for complex long-horizon tasks directly from raw real-world videos, </span> and we reveal that disentangling action dynamics from visual appearance is essential for successful knowledge learning.
            </p>
          </div>

          <div class="highlight-box">
            <p style="font-size: 20px; margin-bottom: 0;">
              2. <b>We propose VideoWorld 2</b> <span style="color: #00f2fe">, whose core is a disentangled Latent Dynamics Model (dLDM) that decouples task-relevant dynamics from visual appearance</span>, enhancing the quality and transferability of learned knowledge.
            </p>
          </div>

          <div class="highlight-box">
            <p style="font-size: 20px; margin-bottom: 0;">
              3. <b>We construct Video-CraftBench</b>, to address the rarely explored challenge of fine-grained, long-horizon visual reasoning through real-world handicraft tasks. This benchmark facilitates future research on learning transferable knowledge from raw videos.
            </p>
          </div>

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop" style="font-family: 'Georgia', serif;">

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">
          <img src="./static/images/videoworld_logo.svg" class="section-logo" alt="Logo" width="5%"/>
          <span class="section-text">Background</span>
        </h2>
      </div>
    </div>
    <div class="columns is-centered ">
      <div class="column is-full">
        <div class="content has-text-justified">
          
          <p style="font-size: 20px;">
            Current AI models  primarily learn  knowledge from large-scale text data . However, text alone cannot fully capture the rich information of the real visual world, including world dynamics, spatial relationships, and underlying physical laws. In contrast, animals in nature can acquire knowledge directly from visual signals, and generalize it to solve tasks across diverse scenarios. For instance, a child can reproduce paper-folding skills demonstrated in a video using different paper materials,  without any language instruction. Given the vast abundance of video content available on the internet, enabling  AI models  to learn generalizable knowledge from raw video data holds significant promise for scaling  their knowledge acquisition and  is fundamental  to their ability to execute tasks effectively in  both real-world and digital environments.
            </p>
            <p style="font-size: 20px;">
            VideoWorld is among the first works to explore  learning knowledge from synthetic videos. It investigates the acquisition of rules, as well as reasoning and planning capabilities,  from Go game records and simulated robotics environments. The study demonstrates that  models can learn such knowledge solely from visual signals using an autoregressive video generation paradigm. However, extending this paradigm beyond synthetic   domains remains an open challenge.  Real-world videos exhibit substantial   visual diversity, complex action  dynamics,  and often involve long-horizon, multi-step interactions.  These characteristics prevent   the training approach and model design of VideoWorld  from being  directly applied to realistic settings. When presented with minute-long, multi-step real-world task videos, VideoWorld  fails to   extract the core task-solving knowledge or generalize it to novel scenarios   through observation alone--even for tasks such as paper folding that are easily mastered by children. These limitations  naturally  lead to the following question:
            <b><i>Can AI models learn transferable knowledge for complex, long-horizon tasks directly from unlabeled real-world videos?</i></b>
          </p>
        </div>
       </div>
    </div>

    <br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">
          <img src="./static/images/videoworld_logo.svg" class="section-logo" alt="Logo" width="5%"/>
          <span class="section-text">Our Work</span>
        </h2>
      </div>
    </div>

    <div class="columns is-centered ">
      <div class="column is-full">
        <div class="content has-text-justified">
          
          <p style="font-size: 20px;">
            To explore this question, we consider two challenging real-world environments. The first is <b>handcraft making</b>, which serves as a strong testbed for learning task knowledge from raw video. These videos require fine-grained manipulation under varied desktop environments and object appearances, involving deformable materials, viewpoint shifts, and frequent occlusions. Furthermore, the videos span minutes and comprise multiple interdependent steps, presenting significantly higher complexity and longer horizons than entertainment-oriented video generation or typical imitation settings. In parallel, we investigate robotic manipulation by learning from the Open-X dataset, which contains real-world demonstration videos, and evaluating on the CALVIN environment to test the generalization of the learned knowledge. Together, these environments provide a comprehensive test of whether knowledge learned from raw videos can transfer across scenes, tasks, and embodiments.
            </p>
            
            <!-- Inserted PDF as Image -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-full">
                <img src="./static/images/supp_key_step_v2.png" class="interpolation-image" alt="Key Steps"/>
                <figcaption style="font-size: 16px; text-align: justify; margin-top: 10px;">Figure 2: Our objective is to learn transferable knowledge for complex and long-horizon tasks from real-world videos. We prioritize tasks that demand multi-step planning and feature delicate manipulations. To this end, we introduce the Video-CraftBench, a dataset of first-person video tutorials covering five long-horizon handcraft tasks: folding a paper airplane, folding a paper boat, and building a tower/horse/person using blocks.</figcaption>
              </div>
            </div>

        </div>
       </div>
    </div>
    <br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">
          <img src="./static/images/videoworld_logo.svg" class="section-logo" alt="Logo" width="5%"/>
          <span class="section-text">Overall Architecture</span>
        </h2>
      </div>
    </div>

   
    <div class="columns is-centered ">
      <div class="column is-full">
        <div class="content has-text-justified">
          <img src="./static/images/method_final.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
                <figcaption style="font-size: 16px;">Figure 3: (Left) First, a dLDM compresses future visual changes into compact and generalizable latent codes. These codes are then modeled by an autoregressive transformer. (Right) In inference, the transformer predicts latent codes for a new, unseen environment from the input image, which are subsequently decoded into task execution videos.</figcaption>
        
          <br>
          <p style="font-size: 20px;">We propose <b>VideoWorld 2</b>, which features a <b>disentangled Latent Dynamics Model</b> (dLDM) as its core design to effectively decouple appearance modeling from action dynamics learning, thereby enabling robust knowledge acquisition. The dLDM consists of a causal VQ-VAE and a pretrained Video Diffusion Model (VDM). The VQ-VAE compresses future visual changes into discrete latent codes that capture task-relevant dynamics, while the VDM models visual appearance and produces high-fidelity reconstructions. The latent codes condition the VDM through cross-attention, and gradients from the VDM further refine these codes so they focus on concise and transferable dynamics rather than appearance details. By delegating appearance modeling to the pretrained VDM, VideoWorld 2 learns more robust and generalizable latent dynamics than prior approaches.</p>
        </div>
      </div>
    </div>

    <br>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">
          <img src="./static/images/videoworld_logo.svg" class="section-logo" alt="Logo" width="5%"/>
          <span class="section-text">dLDM</span>
        </h2>
      </div>
    </div>

    <div class="columns is-centered ">
      <div class="column is-full">
        <div class="content has-text-justified">
          <img src="./static/images/dLDM_final.png"
                class="interpolation-image"
                alt="dLDM architecture"/>
          <figcaption style="font-size: 16px;">The proposed disentangled latent dynamics model (dLDM). (Left) Latent dynamic model in VideoWorld. Visual changes between the first and subsequent frames are compressed into a set of latent codes. (right)  The dLDM proposed in VideoWorld 2. It employs a pre-trained VDM as an appearance prior, yielding better latent codes and facilitating high-fidelity video output.</figcaption>
        </div>
       </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="font-family: 'Georgia', serif;">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{ren2025videoworldexploringknowledgelearning,
  title={VideoWorld: Exploring Knowledge Learning from Unlabeled Videos}, 
  author={Zhongwei Ren and Yunchao Wei and Xun Guo and Yao Zhao and Bingyi Kang and Jiashi Feng and Xiaojie Jin},
  year={2025},
  eprint={2501.09781},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2501.09781}, 
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
Commons Attribution-ShareAlike 4.0 International License</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
$(document).ready(function() {
    // Initialize the carousel with specific options
    var carousels = bulmaCarousel.attach('#results-carousel', {
      slidesToScroll: 1,
      slidesToShow: 1,
      pagination: false,
      loop: true,
      infinite: true,
      autoplay: true,
      autoplaySpeed: 6000, // Slower autoplay to give time for video
      pauseOnHover: true,
    });

    // Video control logic
    if (carousels && carousels.length > 0) {
      var carousel = carousels[0];

      // Function to handle video playback
      function playActiveVideo() {
        // Pause all videos first
        $('#results-carousel video').each(function() {
          this.pause();
        });

        // Find the active item and play its video
        // We use a small timeout because the 'is-active' class might not be updated immediately
        setTimeout(() => {
          var $activeItem = $('#results-carousel .item.is-active');
          var video = $activeItem.find('video').get(0);
          if (video) {
            // Reset time to 0 if you want it to start from beginning every time
            // video.currentTime = 0; 
            var playPromise = video.play();
            if (playPromise !== undefined) {
              playPromise.catch(error => {
                console.log("Auto-play prevented:", error);
              });
            }
          }
        }, 100);
      }

      // Listen for slide changes
      carousel.on('after:show', function(state) {
        console.log("Slide changed", state);
        playActiveVideo();
      });
      
      // Also try 'show' event just in case
      carousel.on('show', function(state) {
        console.log("Slide show", state);
        playActiveVideo();
      });

      // Initial play
      playActiveVideo();
    }

    // --- æ»šåŠ¨åŠ¨ç”»é€»è¾‘ ---
    const observerOptions = {
      root: null,
      rootMargin: '0px',
      threshold: 0.1
    };

    const observer = new IntersectionObserver((entries, observer) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('visible');
          observer.unobserve(entry.target); // åªåŠ¨ç”»ä¸€æ¬¡
        }
      });
    }, observerOptions);

    // è‡ªåŠ¨ç»™ä¸»è¦ç« èŠ‚æ·»åŠ åŠ¨ç”»ç±»
    $('section').each(function() {
      $(this).addClass('scroll-reveal');
      observer.observe(this);
    });
    
    // ç»™ Highlights çš„æ¯ä¸€é¡¹å•ç‹¬æ·»åŠ åŠ¨ç”»ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    $('.highlight-box').each(function(index) {
      $(this).addClass('scroll-reveal');
      $(this).css('transition-delay', (index * 0.1) + 's'); // é”™å³°åŠ¨ç”»
      observer.observe(this);
    });
});
</script>

</body>
</html>
